{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "CNN_VAE_CelebA.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OT5v_ViCvzD",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional VAE on Celeb Face Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLdCLZFaCvzG",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we will go through some experiments using more realistic dataset and we gonna use variational auto-encoder that uses convolutional neural network as encoder/decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1mt9MUrCvzG",
        "colab_type": "text"
      },
      "source": [
        "### Set GPU for Runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH6hrV63CvzH",
        "colab_type": "text"
      },
      "source": [
        "<p id=\"7ecb\" class=\"gy gz ef at ha b hb ja hd jb hf jc hh jd hj je hl\" data-selectable-paragraph=\"\">It is so simple to alter default hardware <strong class=\"ha hm\">(CPU to GPU or vice versa)</strong>; just follow <strong class=\"ha hm\">Edit &gt; Notebook settings</strong> or <strong class=\"ha hm\">Runtime&gt;Change runtime type </strong>and <strong class=\"ha hm\">select GPU </strong>as <strong class=\"ha hm\">Hardware accelerator</strong>.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WidyxnhCvzI",
        "colab_type": "text"
      },
      "source": [
        "![tittle](https://miro.medium.com/max/740/1*WNovJnpGMOys8Rv7YIsZzA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLySuioPCvzJ",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2rOqvKbCvzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import PIL\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMOAol5kCvzM",
        "colab_type": "text"
      },
      "source": [
        "### Globals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99WjQ30ACvzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = 'data'\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "IMG_STATS = [0.5]*3, [0.5]*3 # means and standard deviations for normalizing 3 channel RGB images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfu6kiuHCvzP",
        "colab_type": "text"
      },
      "source": [
        "### Reading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU8YvadxCvzQ",
        "colab_type": "text"
      },
      "source": [
        "We will use CelebA dataset consisting of RGB celebrity face images along with facial characteristic features that we will see below.\n",
        "\n",
        "The dataset can be downloaded from Kaggle:\n",
        "https://www.kaggle.com/jessicali9530/celeba-dataset\n",
        "\n",
        "We will use pre-processed version of the dataset, where pictures have been aligned in center and also excess border has been cropped so that only face occupies the whole image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "87yc1hy3fd_s",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"data\"):\n",
        "    # Get dataset archive\n",
        "    !wget https://maxinai-public-datasets.s3.eu-central-1.amazonaws.com/ms-celeb-images-cropped-aligned.zip\n",
        "    # Unzip directories and files\n",
        "    !unzip ms-celeb-images-cropped-aligned.zip > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MId5zn57CvzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reading image attributes in dataframe\n",
        "\n",
        "df_attr = pd.read_csv(DATA_DIR + '/list_attr_celeba.csv')\n",
        "df_attr.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEVL6B7ECvzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_attr.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpAQeghVCvzY",
        "colab_type": "text"
      },
      "source": [
        "Here we will create a custom PyTorch dataset that will load images, crop them to 64x64 and normalize pixel values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOzfBLgRCvzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CelebFaceDataset(torch.utils.data.dataset.Dataset):\n",
        "    \n",
        "    def __init__(self, root_dir: str, image_ids: List[str]):\n",
        "        \n",
        "        self.root_dir = root_dir\n",
        "        self.image_ids = image_ids\n",
        "        \n",
        "        # original images \n",
        "        self.transforms = transforms.Compose([\n",
        "                                              transforms.Resize((64, 64)),\n",
        "                                              transforms.ToTensor(),\n",
        "                                              transforms.Normalize(*IMG_STATS)\n",
        "                                             ]) \n",
        "        \n",
        "    def __getitem__(self, idx: int):\n",
        "        \n",
        "        return self.get_by_id(self.image_ids[idx])\n",
        "    \n",
        "    def get_by_id(self, image_id: str):\n",
        "        \n",
        "        im = PIL.Image.open(os.path.join(self.root_dir, image_id))\n",
        "\n",
        "        return self.transforms(im)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "def get_celeba_dls(df: pd.DataFrame):\n",
        "    \n",
        "    train_ids, valid_ids = train_test_split(df.image_id.tolist(), test_size=0.01)\n",
        "    \n",
        "    print('Train size:', len(train_ids), 'Valid Size:', len(valid_ids))\n",
        "    \n",
        "    train_ds = CelebFaceDataset(DATA_DIR + '/celeba', train_ids)\n",
        "    valid_ds = CelebFaceDataset(DATA_DIR + '/celeba', valid_ids)\n",
        "    \n",
        "    return (torch.utils.data.DataLoader(train_ds, \n",
        "                                        batch_size=BATCH_SIZE, \n",
        "                                        shuffle=True, \n",
        "                                        pin_memory=True),\n",
        "            \n",
        "            torch.utils.data.DataLoader(valid_ds, \n",
        "                                        batch_size=2 * BATCH_SIZE, \n",
        "                                        shuffle=False, \n",
        "                                        pin_memory=True))\n",
        "\n",
        "train_dl, valid_dl = get_celeba_dls(df_attr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hiaGXS6Cvzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is a helper class that is used to apply inverse of data normalization so that \n",
        "# images return to valid [0, 1] range necessary fot visualization.\n",
        "\n",
        "class DeNormalize:\n",
        "    \n",
        "    def __init__(self, mean, std):\n",
        "        \n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, x, inplace=False):\n",
        "        \n",
        "        tensor = x if inplace else x.clone() \n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m)\n",
        "        \n",
        "        return tensor\n",
        "    \n",
        "denorm = DeNormalize(*IMG_STATS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhnL2UuICvzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grabbing single batch and displaying some stats\n",
        "\n",
        "x = next(iter(train_dl))\n",
        "x.shape, x.mean(), x.std(), x.min(), x.max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC5uysu4Cvzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualizing bunch of images from our dataset\n",
        "\n",
        "grid = denorm(make_grid(x[0:50,...], padding=1, pad_value=1, nrow=10).detach()).numpy()\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(grid.transpose(1, 2, 0))\n",
        "plt.axis('off');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpNhPOQKCvzh",
        "colab_type": "text"
      },
      "source": [
        "### Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKANhf59Cvzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function that initializes weights of Convolutional and Linear layers \n",
        "# https://arxiv.org/abs/1502.01852\n",
        "\n",
        "def init_weights(m: nn.Module):\n",
        "\n",
        "    for m in m.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            if m.bias is not None: nn.init.constant_(m.bias, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5pkykceCvzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function that creates a convolutional block followed by batch normalization and relu activation\n",
        "\n",
        "def conv(nf: int, of: int, ks: int, stride: int = 1, use_act: bool = True, use_bn: bool = True):\n",
        "    \n",
        "    conv = nn.Conv2d(in_channels=nf, \n",
        "                     out_channels=of, \n",
        "                     kernel_size=ks, \n",
        "                     stride=stride, \n",
        "                     padding=ks // 2, \n",
        "                     bias=not use_bn)\n",
        "    \n",
        "    layers = [conv]\n",
        "    \n",
        "    if use_bn:\n",
        "        layers.append(nn.BatchNorm2d(of))\n",
        "    \n",
        "    if use_act:\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "    \n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "conv(3, 64, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_I0-c1iCvzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels: int, nf: int):\n",
        "        super().__init__()\n",
        "        \n",
        "        layers = []\n",
        "        \n",
        "        layers.append(conv(in_channels, nf, ks=3))\n",
        "        layers.append(conv(nf, nf, ks=3))\n",
        "        for i in range(3):\n",
        "            layers.append(conv(nf, 2*nf, ks=3, stride=2))\n",
        "            layers.append(conv(2*nf, 2*nf, ks=3, stride=1))\n",
        "            nf *= 2\n",
        "        \n",
        "        self.encoder = nn.Sequential(*layers)\n",
        "        \n",
        "        init_weights(self)\n",
        "        \n",
        "    def forward(self, x): return self.encoder(x)\n",
        "    \n",
        "Encoder(3, 32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZY5s8JkCvzo",
        "colab_type": "text"
      },
      "source": [
        "In decoder, we will be using nearest-neighbour upscaling followed by convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEqkFMQtCvzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels: int, nf: int):\n",
        "        super().__init__()\n",
        "        \n",
        "        layers = []\n",
        "        \n",
        "        for i in range(3):\n",
        "            layers.append(nn.UpsamplingNearest2d(scale_factor=2))\n",
        "            layers.append(conv(nf, nf//2, ks=3, stride=1))\n",
        "            nf //= 2\n",
        "        \n",
        "        layers.append(conv(nf, in_channels, ks=1, use_act=False, use_bn=False))\n",
        "        \n",
        "        self.decoder = nn.Sequential(*layers)\n",
        "        \n",
        "        init_weights(self)\n",
        "    \n",
        "    def forward(self, x): return self.decoder(x)\n",
        "    \n",
        "Decoder(3, 256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5PxF6P3Cvzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvolutionalVAE(nn.Module):\n",
        "    \n",
        "    def __init__(self, z_dim: int):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.enc_out_shape = (256, 8, 8)\n",
        "            \n",
        "        self.encoder = Encoder(3, 32)\n",
        "        self.decoder = Decoder(3, self.enc_out_shape[0])\n",
        "\n",
        "        self.mu     = nn.Linear(np.prod(self.enc_out_shape), z_dim)\n",
        "        self.logvar = nn.Linear(np.prod(self.enc_out_shape), z_dim)\n",
        "        \n",
        "        self.fc = nn.Linear(z_dim, np.prod(self.enc_out_shape))\n",
        "        \n",
        "        init_weights(self)\n",
        "        \n",
        "    def encode(self, x):\n",
        "        \n",
        "        enc = self.encoder(x).view(x.shape[0], -1)\n",
        "        \n",
        "        mu, logvar = self.mu(enc), self.logvar(enc)\n",
        "        \n",
        "        return mu, logvar\n",
        "    \n",
        "    def sample_z(self, mu, logvar):\n",
        "        \n",
        "        eps = torch.rand_like(mu)\n",
        "        \n",
        "        return mu + eps * torch.exp(0.5 * logvar)\n",
        "    \n",
        "    def decode(self, z):\n",
        "        \n",
        "        # tanh activation is used to squeeze output pixel values in [-1, 1] range similar to input.\n",
        "        \n",
        "        return torch.tanh(self.decoder(self.fc(z).view(-1, *self.enc_out_shape)))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        mu, logvar = self.encode(x)\n",
        "        \n",
        "        z = self.sample_z(mu, logvar)\n",
        "        \n",
        "        return self.decode(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReXkL5PhCvzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "o = ConvolutionalVAE(10)(x)\n",
        "o.shape, o.mean(), o.std(), o.min(), o.max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhWVBHPhCvzv",
        "colab_type": "text"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4NagzMFCvzv",
        "colab_type": "text"
      },
      "source": [
        "Below we build a training loop.  \n",
        "\n",
        "Instead of L2 term in ELBO loss, we use L1 loss that produces not as blurry outputs as L2 does (recommended by Jeremy Howard from fast.ai). \n",
        "\n",
        "We use Adam optimizer with learning learning decay of factor 2 after every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfhlRVPqCvzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def elbo_loss(inp, tar, mu, logvar, alpha: float = 1, beta: float = 1):\n",
        "    \n",
        "    recon_loss = nn.functional.l1_loss(inp, tar, reduction='none').sum(dim=(1, 2, 3))\n",
        "            \n",
        "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=-1)\n",
        "\n",
        "    loss = torch.mean(alpha * kld_loss + beta * recon_loss)\n",
        "    \n",
        "    return loss, torch.mean(recon_loss), torch.mean(kld_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7P5b45vCvzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_summary(valid_dl: DataLoader, model: DataLoader):\n",
        "    \n",
        "    N_SAMPLES = 15\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    actuals, preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for i, x in enumerate(valid_dl.dataset):\n",
        "            \n",
        "            actuals.append(x)\n",
        "            \n",
        "            recon_x = model(x.unsqueeze(0).cuda()).cpu()\n",
        "            \n",
        "            preds.append(recon_x.squeeze(0))\n",
        "            \n",
        "            if i + 1 == N_SAMPLES:\n",
        "                break\n",
        "                \n",
        "    model.train()\n",
        "            \n",
        "    grid = denorm(make_grid([*actuals, *preds], pad_value=1, padding=1, nrow=N_SAMPLES))\n",
        "\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    plt.imshow(grid.permute(1, 2, 0))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def train_vae(train_dl: DataLoader, \n",
        "              valid_dl: DataLoader,\n",
        "              model: nn.Module,\n",
        "              n_epochs: int):\n",
        "    \n",
        "    LOG_INTERVAL = 20\n",
        "    SUMMARY_INTERVAL = 100\n",
        "    \n",
        "    model = model.cuda()\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    \n",
        "    sched = torch.optim.lr_scheduler.StepLR(optim, step_size=1, gamma=0.5, last_epoch=-1)\n",
        "    \n",
        "    acc_recon_loss, acc_kld_loss, acc_loss = 0, 0, 0\n",
        "    \n",
        "    i = 1\n",
        "    for epoch in range(n_epochs):\n",
        "        for x in train_dl:\n",
        "            \n",
        "            optim.zero_grad()\n",
        "            \n",
        "            x = x.cuda()\n",
        "\n",
        "            mu, logvar = model.encode(x)\n",
        "            \n",
        "            z = model.sample_z(mu, logvar)\n",
        "        \n",
        "            x_recon = model.decode(z)\n",
        "        \n",
        "            loss, recon_loss, kld_loss = elbo_loss(x_recon, x, mu, logvar, alpha=1, beta=1)\n",
        "\n",
        "            acc_recon_loss += recon_loss.item()\n",
        "            acc_kld_loss += kld_loss.item()\n",
        "            acc_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            optim.step()\n",
        "            \n",
        "            if (i + 1) % LOG_INTERVAL == 0:\n",
        "                \n",
        "                print('epoch %d | iter %d | loss %.5f | KL loss %.5f | recon loss %.5f' % \n",
        "                      (epoch + 1, \n",
        "                       i + 1, \n",
        "                       acc_loss / LOG_INTERVAL, \n",
        "                       acc_kld_loss / LOG_INTERVAL, \n",
        "                       acc_recon_loss / LOG_INTERVAL,\n",
        "                       ))\n",
        "                \n",
        "                acc_recon_loss, acc_kld_loss, acc_loss = 0, 0, 0\n",
        "                \n",
        "            if (i + 1) % SUMMARY_INTERVAL == 0:\n",
        "                show_summary(valid_dl, model)\n",
        "                \n",
        "            i += 1\n",
        "            \n",
        "        sched.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyrzjyCfCvz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will use 128 dimensional latent space for this task.\n",
        "\n",
        "vae = ConvolutionalVAE(128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYO70AGgCvz4",
        "colab_type": "text"
      },
      "source": [
        "As far as it takes a while until network starts producing good quality outputs, we will be using pre-trained weights for visualizations in this notebook.  \n",
        "  \n",
        "  \n",
        "You can still uncomment and run the following cell for few iterations to see how output quality gradually increases as training proceedes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c49bnGUHCvz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_vae(train_dl, valid_dl, vae, 50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81QLH77_Cvz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.save(vae.state_dict(), 'cnn-vae')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glGaxkj5EL-0",
        "colab_type": "text"
      },
      "source": [
        "### Transfer Learning & Perceptual Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ubhj3OfEQ8i",
        "colab_type": "text"
      },
      "source": [
        "There are bunch of tricks we could use to achieve better performance on our task. For instance we could experiment with other encoders (e.g. Resnet, EfficientNet), try other upsampling strategies rather than nearest-neighbour (e.g Transposed Convolution, Subpixcel Convolution, etc...)  \n",
        "  \n",
        "Here we will show one, rather non-intuitive, strategy that will reduce blurness of outputs and speed up convergence of our CNN VAE.  \n",
        "\n",
        "This method is using third loss component - <b>Perceptual Loss</b> https://arxiv.org/abs/1603.08155  \n",
        "\n",
        "The brief idea is to take frozen ImageNet pre-trained CNN model (here VGG16), do forward-pass on given training image as well as reconstruction produced by VAE, and enforce certain intermediate feature maps of both forward passed to be close together by L1/L2 losses.  \n",
        "\n",
        "Intuition is that, we not only enforce image similarity in original pixel space (that is, MSE(input, reconstruction)), but also in intermediary feature spaces of VGG16."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaP5k4EgESkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining perceptual loss using pretrained VGG16 architecture.\n",
        "# code partially taken from fast.ai deep learning course\n",
        "\n",
        "class Hook:\n",
        "    \"\"\"Helper class to capture intermediary feature activations during forward-pass in VGG16.\"\"\"\n",
        "\n",
        "    def __init__(self, m): \n",
        "        \n",
        "        self.feats = None\n",
        "        \n",
        "        self.hook = m.register_forward_hook(self._hook_fn)\n",
        "    \n",
        "    def _hook_fn(self, m, inp, out): \n",
        "        self.feats = out\n",
        "        \n",
        "    def remove(self): \n",
        "        self.hook.remove()        \n",
        "\n",
        "class PerceptualLoss:\n",
        "    \"\"\"Perceptual Loss using VGG16 architecture.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        vgg16 = torchvision.models.vgg16_bn(pretrained=True)\n",
        "        \n",
        "        self.vgg16_head = nn.Sequential(*list(vgg16.children())[0]).cuda()\n",
        "                \n",
        "        # These are the indices of layers in VGG16 we are going to use in our loss. \n",
        "        feat_indices = [5, 15]\n",
        "        \n",
        "        # freezing the model. We only need static VGG16 used only for forward-pass.\n",
        "        for p in self.vgg16_head.parameters():\n",
        "            p.requires_grad = False\n",
        "        \n",
        "        # registering PyTorch hooks that will grab feature activations during forward pass.\n",
        "        self.hooks = [Hook(self.vgg16_head[i]) for i in feat_indices]\n",
        "        \n",
        "    def __call__(self, x_recon, x):\n",
        "        \n",
        "        # forward-pass on reconstructed image.\n",
        "        self.vgg16_head(x_recon)\n",
        "        \n",
        "        # saving extracted feature activations.\n",
        "        x_recon_feats = [h.feats.clone() for h in self.hooks]\n",
        "        \n",
        "        # forward-pass on original input image.\n",
        "        self.vgg16_head(x)\n",
        "        \n",
        "        # applying MSE loss on extracted features.\n",
        "        loss = sum([torch.nn.functional.mse_loss(x_recon_feats[i], h.feats, reduction='sum') for i, h in enumerate(self.hooks)])\n",
        "        \n",
        "        return loss\n",
        "            \n",
        "    def close(self):\n",
        "        for h in self.hooks: h.remove()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2avmiogXEgFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "percept_loss = PerceptualLoss()\n",
        "percept_loss.vgg16_head"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9EwAsgsEoH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we redefine our ELBO loss so that it now includes perceptual loss component as well\n",
        "\n",
        "def elbo_loss(inp, tar, mu, logvar, alpha: float = 1, beta: float = 1, gamma: float = 1):\n",
        "    \n",
        "    recon_loss = nn.functional.l1_loss(inp, tar, reduction='none').sum(dim=(1, 2, 3))\n",
        "            \n",
        "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=-1)\n",
        "\n",
        "    loss = torch.mean(alpha * kld_loss + beta * recon_loss + gamma * percept_loss(inp, tar))\n",
        "    \n",
        "    return loss, torch.mean(recon_loss), torch.mean(kld_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50clgsVNFya2",
        "colab_type": "text"
      },
      "source": [
        "We can now train our VAE with Perceptual loss and see that it gives better results on early iterations as opposed to previous training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m42gyWMEoMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae_percept = ConvolutionalVAE(128)\n",
        "\n",
        "train_vae(train_dl, valid_dl, vae_percept, 50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxYgtj9QCv0A",
        "colab_type": "text"
      },
      "source": [
        "## Experiments with Visual Arithmetic in Latent Space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lc_gfObCv0B",
        "colab_type": "text"
      },
      "source": [
        "Similar to word2vec, we can do some pretty interesting visual experiments by performing arithmetics in latent space.\n",
        "\n",
        "For example, we can manipulate certain facial attributes (e.g. wearing sunglasses, having a smile, long hair, etc...) on a picture by simply adding/subtracting the some vectors.\n",
        "\n",
        "Below we will show how to paint sunglasses and smile on an arbitrary image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7owv0kwCvz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('cnn-vae-5-l1.model'):\n",
        "    !wget https://maxinai-public-datasets.s3.eu-central-1.amazonaws.com/workshop-amld2020/cnn-vae-5-l1.model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QzFe_DJCvz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# here we load pre-trained VAE for further experiments.\n",
        "\n",
        "vae.load_state_dict(torch.load('cnn-vae-5-l1.model'))\n",
        "vae.eval();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDwia45VCv0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function selects 500 images having a certain characteristic (e.g Glasses, Long Hair, etc...) and averages their\n",
        "# latent vectors in order to get a vector encoding this characteristic.\n",
        "\n",
        "def get_mean_vec(vae: nn.Module, df: pd.DataFrame, condition):\n",
        "\n",
        "    vae.cpu()\n",
        "    \n",
        "    images = []\n",
        "    for i, im_id in enumerate(df[condition].image_id.values):\n",
        "        images.append(valid_dl.dataset.get_by_id(im_id))\n",
        "        if i >= 250:\n",
        "            break\n",
        "        \n",
        "    images = torch.stack(images)\n",
        "    \n",
        "    mu, logvar = vae.encode(images)\n",
        "        \n",
        "    return torch.mean(vae.sample_z(mu, logvar), dim=0, keepdim=True)\n",
        " \n",
        "# computing mean vectors of \"wearing glasses\" and \"not wearing glasses\"\n",
        "glasses1 = get_mean_vec(vae, df_attr, (df_attr.Eyeglasses == 1))\n",
        "glasses0 = get_mean_vec(vae, df_attr, (df_attr.Eyeglasses == -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-ySEpQbCv0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualizing average face with glasses\n",
        "\n",
        "im = denorm(vae.decode(glasses1).detach())[0].numpy()\n",
        "plt.imshow(im.transpose(1,2,0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iENwZygnCv0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualizing average face without glasses\n",
        "\n",
        "im = denorm(vae.decode(glasses0).detach())[0].numpy()\n",
        "plt.imshow(im.transpose(1,2,0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpH__gvwCv0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizing the Vector(\"wearing glasses\") - Vector(\"not wearing glasses\") that gives vector encoding glass information.\n",
        "\n",
        "im = denorm(vae.decode(glasses1 - glasses0).detach())[0].numpy()\n",
        "plt.imshow(im.transpose(1,2,0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsWkk-g5Cv0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we are going to use above computed vectors to generate images with glasses.\n",
        "\n",
        "images = []\n",
        "for i, im_id in enumerate(df_attr[df_attr.Eyeglasses == -1].tail(10).image_id.values):\n",
        "    images.append(valid_dl.dataset.get_by_id(im_id))\n",
        "images = torch.stack(images)\n",
        "\n",
        "mu, logvar = vae.encode(images)\n",
        "\n",
        "z1 = vae.sample_z(mu, logvar)\n",
        "z2 = vae.sample_z(mu + 2.5*(glasses1 - glasses0), logvar)\n",
        "\n",
        "images = torch.cat([vae.decode(z1), vae.decode(z2)])\n",
        "\n",
        "grid = denorm(make_grid(images, padding=1, pad_value=1, nrow=10).detach()).numpy()\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(grid.transpose(1, 2, 0))\n",
        "plt.axis('off');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwfmIlboCv0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can do the same experiment with \"smile vector\"\n",
        "\n",
        "smile1 = get_mean_vec(vae, df_attr, (df_attr.Smiling == 1))\n",
        "smile0 = get_mean_vec(vae, df_attr, (df_attr.Smiling == -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lKi3ofuCv0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im = denorm(vae.decode(smile1).detach())[0].numpy()\n",
        "plt.imshow(im.transpose(1,2,0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_83lSJ5Cv0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im = denorm(vae.decode(smile0).detach())[0].numpy()\n",
        "plt.imshow(im.transpose(1,2,0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_ITmuLCv0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = []\n",
        "for i, im_id in enumerate(df_attr[df_attr.Smiling == -1].tail(10).image_id.values):\n",
        "    images.append(valid_dl.dataset.get_by_id(im_id))\n",
        "images = torch.stack(images)\n",
        "\n",
        "mu, logvar = vae.encode(images)\n",
        "\n",
        "z1 = vae.sample_z(mu, logvar)\n",
        "z2 = vae.sample_z(mu + 2*(smile1 - smile0), logvar)\n",
        "\n",
        "images = torch.cat([vae.decode(z1), vae.decode(z2)])\n",
        "\n",
        "grid = denorm(make_grid(images, padding=1, pad_value=1, nrow=10).detach()).numpy()\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(grid.transpose(1, 2, 0))\n",
        "plt.axis('off');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSsJGcoVCv0Q",
        "colab_type": "text"
      },
      "source": [
        "Now we will visualize the 2D manifold where axes encode \"Smile\" and \"Wearing Sunglasses\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_HIAPyjCv0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mean Z vector of no smile and with eyeglasses\n",
        "z_smile0_glass1 = get_mean_vec(vae, df_attr, (df_attr.Eyeglasses == 1) & (df_attr.Smiling == -1))\n",
        "\n",
        "# mean Z vector of smile and with no eyeglasses\n",
        "z_smile1_glass0 = get_mean_vec(vae, df_attr, (df_attr.Eyeglasses == -1) & (df_attr.Smiling == 1))\n",
        "\n",
        "# mean Z vector of no smile and with no eyeglasses\n",
        "z_glass0_smile0 = get_mean_vec(vae, df_attr, (df_attr.Eyeglasses == -1) & (df_attr.Smiling == -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LefjLLOECv0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "line = np.linspace(0, 2, 15)\n",
        "\n",
        "# definining 2D basis vectors\n",
        "I, J = z_smile0_glass1 - z_glass0_smile0, z_smile1_glass0 - z_glass0_smile0\n",
        "\n",
        "res = []\n",
        "for a in line:\n",
        "    for b in line:\n",
        "        z = z_glass0_smile0 + a * I + b * J\n",
        "        x_recon = denorm(vae.decode(z))\n",
        "        res.append(x_recon)\n",
        "        \n",
        "# displaying space\n",
        "\n",
        "grd = make_grid(torch.cat(res, dim=0), 15).detach().numpy()\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(grd.transpose(1, 2, 0))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}